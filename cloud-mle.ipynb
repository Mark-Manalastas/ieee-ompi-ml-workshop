{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Machine Learning Engine\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "- [The Cloud MLE Train/Deploy Process](#the-cloud-mle-traindeploy-process)\n",
    "- [Preparing for Training and Serving on Cloud MLE](#preparing-for-training-and-serving-on-cloud-mle)\n",
    "- [Packaging the Code for Training on CloudMLE](#packaging-the-code-for-training-on-cloudmle)\n",
    "- [The TensorFlow Model](#the-tensorflow-model)\n",
    "- [The Application Logic](#the-application-logic)\n",
    "- [Training on CloudMLE](#training-on-cloudmle)\n",
    "\n",
    "The Google Cloud Machine Learning Engine, simply known as Cloud MLE is a managed Google infrastructure for training and serving “large-scale” machine learning models. This managed infrastructure can train large scale Machine Learning models built with TensorFlow, Keras, Scikit-learn or XGBoost. It also provides modes of serving or consuming the trained models either as an Online or Batched prediction service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"google-machine-learning-engine\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Cloud MLE Train/Deploy Process\n",
    "1. The data for training/ inference is kept on GCS.\n",
    "2. The execution script uses the application logic to train the model on Cloud MLE using the training data.\n",
    "3. The trained model is stored on GCS.\n",
    "4. A prediction service is created on Cloud MLE using the trained model.\n",
    "5. The external application send data to the deployed model for inference.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/cloud-mle.png\" style=\"float:left;\" alt=\"The Train/Deploy process on CloudMLE.\" height=90% width=90% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"preparing-for-training-and-serving-on-cloud-mle\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for Training and Serving on Cloud MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bucket on GCS.\n",
    "gsutil mb gs://iris-dataset-ieee-ompi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move training data\n",
    "curl https://raw.githubusercontent.com/dvdbisong/gcp-learningmodels-book/master/Chapter_44/tensorflow/train_data.csv | gsutil cp - gs://iris-dataset-ieee-ompi/train_data.csv        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move test data\n",
    "curl https://raw.githubusercontent.com/dvdbisong/gcp-learningmodels-book/master/Chapter_44/tensorflow/test_data.csv | gsutil cp - gs://iris-dataset-ieee-ompi/test_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move hold out data for batch predictions\n",
    "curl https://raw.githubusercontent.com/dvdbisong/gcp-learningmodels-book/master/Chapter_44/tensorflow/hold_out_test.csv | gsutil cp - gs://iris-dataset-ieee-ompi/hold_out_test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Cloud MLE API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable cloud MLE API\n",
    "gcloud services enable ml.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging the Code for Training on Cloud MLE\n",
    "\n",
    "The code for training on Cloud MLE must be prepared as a python package. The recommended project structure is explained as follows:\n",
    "\n",
    "**IrisCloudML** - [project name as parent folder] <br>\n",
    "- trainer - [folder containing the model and execution code]\n",
    "  - init .py - [an empty special python file indicating that the containing folder is a Python package].\n",
    "  - model.py - [script contains the logic of the model written in TensorFlow, Keras, etc.]\n",
    "  - task.py - [script contains the application that orchestrates or manages the training\n",
    "job]\n",
    "- scripts - [folder containing scripts to execute jobs on Cloud ML]\n",
    "  - distributed-training.sh - [script to run a distributed training job on Cloud MLE].\n",
    "  - hyper-tune.sh - [script to run a training job with hyper-parameter tuning on Cloud MLE].\n",
    "  - single-instance-training.sh - [script to run a single instance training job on Cloud MLE].\n",
    "  - online-prediction.sh - [script to execute an online prediction job on Cloud MLE].\n",
    "  - create-prediction-service.sh [script to create a prediction service on Cloud MLE].\n",
    "- hptuning config - [onfiguration file for hyper-parameter tuning on Cloud MLE]\n",
    "- gpu hptuning config.yaml - [configuration file for hyper-parameter tuning with GPU training on Cloud MLE]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TensorFlow Model\n",
    "The TF Model code in he file model.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile trainer/model.py\n",
    "import six\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
    "\n",
    "# Define the format of your input data including unused columns.\n",
    "CSV_COLUMNS = [\n",
    "    'sepal_length', 'sepal_width', 'petal_length',\n",
    "    'petal_width', 'class'\n",
    "]\n",
    "CSV_COLUMN_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], ['']]\n",
    "LABEL_COLUMN = 'class'\n",
    "LABELS = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "# Define the initial ingestion of each feature used by your model.\n",
    "# Additionally, provide metadata about the feature.\n",
    "INPUT_COLUMNS = [\n",
    "    # Continuous base columns.\n",
    "    tf.feature_column.numeric_column('sepal_length'),\n",
    "    tf.feature_column.numeric_column('sepal_width'),\n",
    "    tf.feature_column.numeric_column('petal_length'),\n",
    "    tf.feature_column.numeric_column('petal_width')\n",
    "]\n",
    "\n",
    "UNUSED_COLUMNS = set(CSV_COLUMNS) - {col.name for col in INPUT_COLUMNS} - \\\n",
    "    {LABEL_COLUMN}\n",
    "\n",
    "def build_estimator(config, hidden_units=None, learning_rate=None):\n",
    "    \"\"\"Deep NN Classification model for predicting flower class.\n",
    "    Args:\n",
    "        config: (tf.contrib.learn.RunConfig) defining the runtime environment for\n",
    "            the estimator (including model_dir).\n",
    "        hidden_units: [int], the layer sizes of the DNN (input layer first)\n",
    "        learning_rate: (int), the learning rate for the optimizer.\n",
    "    Returns:\n",
    "        A DNNClassifier\n",
    "    \"\"\"\n",
    "    (sepal_length, sepal_width, petal_length, petal_width) = INPUT_COLUMNS\n",
    "\n",
    "    columns = [\n",
    "        sepal_length,\n",
    "        sepal_width,\n",
    "        petal_length,\n",
    "        petal_width,\n",
    "    ]\n",
    "\n",
    "    return tf.estimator.DNNClassifier(\n",
    "      config=config,\n",
    "      feature_columns=columns,\n",
    "      hidden_units=hidden_units or [256, 128, 64],\n",
    "      n_classes = 3,\n",
    "      optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "    )\n",
    "\n",
    "def parse_label_column(label_string_tensor):\n",
    "  \"\"\"Parses a string tensor into the label tensor.\n",
    "  Args:\n",
    "    label_string_tensor: Tensor of dtype string. Result of parsing the CSV\n",
    "      column specified by LABEL_COLUMN.\n",
    "  Returns:\n",
    "    A Tensor of the same shape as label_string_tensor, should return\n",
    "    an int64 Tensor representing the label index for classification tasks,\n",
    "    and a float32 Tensor representing the value for a regression task.\n",
    "  \"\"\"\n",
    "  # Build a Hash Table inside the graph\n",
    "  table = tf.contrib.lookup.index_table_from_tensor(tf.constant(LABELS))\n",
    "\n",
    "  # Use the hash table to convert string labels to ints and one-hot encode\n",
    "  return table.lookup(label_string_tensor)\n",
    "\n",
    "# [START serving-function]\n",
    "\n",
    "def csv_serving_input_fn():\n",
    "    \"\"\"Build the serving inputs.\"\"\"\n",
    "    csv_row = tf.placeholder(shape=[None], dtype=tf.string)\n",
    "    features = _decode_csv(csv_row)\n",
    "    # Ignore label column\n",
    "    features.pop(LABEL_COLUMN)\n",
    "    return tf.estimator.export.ServingInputReceiver(features,\n",
    "                                              {'csv_row': csv_row})\n",
    "\n",
    "def example_serving_input_fn():\n",
    "    \"\"\"Build the serving inputs.\"\"\"\n",
    "    example_bytestring = tf.placeholder(\n",
    "      shape=[None],\n",
    "      dtype=tf.string,\n",
    "    )\n",
    "    features = tf.parse_example(\n",
    "      example_bytestring,\n",
    "      tf.feature_column.make_parse_example_spec(INPUT_COLUMNS))\n",
    "    return tf.estimator.export.ServingInputReceiver(\n",
    "      features, {'example_proto': example_bytestring})\n",
    "\n",
    "def json_serving_input_fn():\n",
    "    \"\"\"Build the serving inputs.\"\"\"\n",
    "    inputs = {}\n",
    "    for feat in INPUT_COLUMNS:\n",
    "        inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
    "\n",
    "# [END serving-function]\n",
    "\n",
    "SERVING_FUNCTIONS = {\n",
    "  'JSON': json_serving_input_fn,\n",
    "  'EXAMPLE': example_serving_input_fn,\n",
    "  'CSV': csv_serving_input_fn\n",
    "}\n",
    "\n",
    "def _decode_csv(line):\n",
    "    \"\"\"Takes the string input tensor and returns a dict of rank-2 tensors.\"\"\"\n",
    "\n",
    "    # Takes a rank-1 tensor and converts it into rank-2 tensor\n",
    "    row_columns = tf.expand_dims(line, -1)\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "    features = dict(zip(CSV_COLUMNS, columns))\n",
    "\n",
    "    # Remove unused columns\n",
    "    for col in UNUSED_COLUMNS:\n",
    "      features.pop(col)\n",
    "    return features\n",
    "\n",
    "def input_fn(filenames,\n",
    "         num_epochs=None,\n",
    "         shuffle=True,\n",
    "         skip_header_lines=1,\n",
    "         batch_size=200):\n",
    "    \"\"\"Generates features and labels for training or evaluation.\n",
    "    This uses the input pipeline based approach using file name queue\n",
    "    to read data so that entire data is not loaded in memory.\n",
    "    Args:\n",
    "      filenames: [str] A List of CSV file(s) to read data from.\n",
    "      num_epochs: (int) how many times through to read the data. If None will\n",
    "        loop through data indefinitely\n",
    "      shuffle: (bool) whether or not to randomize the order of data. Controls\n",
    "        randomization of both file order and line order within files.\n",
    "      skip_header_lines: (int) set to non-zero in order to skip header lines in\n",
    "        CSV files.\n",
    "      batch_size: (int) First dimension size of the Tensors returned by input_fn\n",
    "    Returns:\n",
    "      A (features, indices) tuple where features is a dictionary of\n",
    "        Tensors, and indices is a single Tensor of label indices.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TextLineDataset(filenames).skip(skip_header_lines).map(\n",
    "      _decode_csv)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=batch_size * 10)\n",
    "    iterator = dataset.repeat(num_epochs).batch(\n",
    "        batch_size).make_one_shot_iterator()\n",
    "    features = iterator.get_next()\n",
    "    return features, parse_label_column(features.pop(LABEL_COLUMN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logic File\n",
    "The application logic in the file `task.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile trainer/task.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "import trainer.model as model\n",
    "\n",
    "def _get_session_config_from_env_var():\n",
    "    \"\"\"Returns a tf.ConfigProto instance that has appropriate device_filters set.\n",
    "    \"\"\"\n",
    "\n",
    "    tf_config = json.loads(os.environ.get('TF_CONFIG', '{}'))\n",
    "\n",
    "    if (tf_config and 'task' in tf_config and 'type' in tf_config['task'] and\n",
    "       'index' in tf_config['task']):\n",
    "        # Master should only communicate with itself and ps\n",
    "        if tf_config['task']['type'] == 'master':\n",
    "            return tf.ConfigProto(device_filters=['/job:ps', '/job:master'])\n",
    "        # Worker should only communicate with itself and ps\n",
    "        elif tf_config['task']['type'] == 'worker':\n",
    "            return tf.ConfigProto(device_filters=[\n",
    "                '/job:ps',\n",
    "                '/job:worker/task:%d' % tf_config['task']['index']\n",
    "            ])\n",
    "    return None\n",
    "\n",
    "def train_and_evaluate(hparams):\n",
    "    \"\"\"Run the training and evaluate using the high level API.\"\"\"\n",
    "\n",
    "    train_input = lambda: model.input_fn(\n",
    "        hparams.train_files,\n",
    "        num_epochs=hparams.num_epochs,\n",
    "        batch_size=hparams.train_batch_size\n",
    "    )\n",
    "\n",
    "    # Don't shuffle evaluation data\n",
    "    eval_input = lambda: model.input_fn(\n",
    "        hparams.eval_files,\n",
    "        batch_size=hparams.eval_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        train_input, max_steps=hparams.train_steps)\n",
    "\n",
    "    exporter = tf.estimator.FinalExporter(\n",
    "        'iris', model.SERVING_FUNCTIONS[hparams.export_format])\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        eval_input,\n",
    "        steps=hparams.eval_steps,\n",
    "        exporters=[exporter],\n",
    "        name='iris-eval')\n",
    "\n",
    "    run_config = tf.estimator.RunConfig(\n",
    "        session_config=_get_session_config_from_env_var())\n",
    "    run_config = run_config.replace(model_dir=hparams.job_dir)\n",
    "    print('Model dir %s' % run_config.model_dir)\n",
    "    estimator = model.build_estimator(\n",
    "        learning_rate=hparams.learning_rate,\n",
    "        # Construct layers sizes with exponential decay\n",
    "        hidden_units=[\n",
    "            max(2, int(hparams.first_layer_size * hparams.scale_factor**i))\n",
    "            for i in range(hparams.num_layers)\n",
    "        ],\n",
    "        config=run_config)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Input Arguments\n",
    "    parser.add_argument(\n",
    "        '--train-files',\n",
    "        help='GCS file or local paths to training data',\n",
    "        nargs='+',\n",
    "        default='gs://iris-dataset/train_data.csv')\n",
    "    parser.add_argument(\n",
    "        '--eval-files',\n",
    "        help='GCS file or local paths to evaluation data',\n",
    "        nargs='+',\n",
    "        default='gs://iris-dataset/test_data.csv')\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help='GCS location to write checkpoints and export models',\n",
    "        default='/tmp/iris-estimator')\n",
    "    parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help=\"\"\"\\\n",
    "        Maximum number of training data epochs on which to train.\n",
    "        If both --max-steps and --num-epochs are specified,\n",
    "        the training job will run for --max-steps or --num-epochs,\n",
    "        whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "        \"\"\",\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--train-batch-size',\n",
    "        help='Batch size for training steps',\n",
    "        type=int,\n",
    "        default=20)\n",
    "    parser.add_argument(\n",
    "        '--eval-batch-size',\n",
    "        help='Batch size for evaluation steps',\n",
    "        type=int,\n",
    "        default=20)\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        help='The training learning rate',\n",
    "        default=1e-4,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--first-layer-size',\n",
    "        help='Number of nodes in the first layer of the DNN',\n",
    "        default=256,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--num-layers', help='Number of layers in the DNN', default=3, type=int)\n",
    "    parser.add_argument(\n",
    "        '--scale-factor',\n",
    "        help='How quickly should the size of the layers in the DNN decay',\n",
    "        default=0.7,\n",
    "        type=float)\n",
    "    parser.add_argument(\n",
    "        '--train-steps',\n",
    "        help=\"\"\"\\\n",
    "        Steps to run the training job for. If --num-epochs is not specified,\n",
    "        this must be. Otherwise the training job will run indefinitely.\\\n",
    "        \"\"\",\n",
    "        default=100,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--eval-steps',\n",
    "        help='Number of steps to run evalution for at each checkpoint',\n",
    "        default=100,\n",
    "        type=int)\n",
    "    parser.add_argument(\n",
    "        '--export-format',\n",
    "        help='The input format of the exported SavedModel binary',\n",
    "        choices=['JSON', 'CSV', 'EXAMPLE'],\n",
    "        default='CSV')\n",
    "    parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=['DEBUG', 'ERROR', 'FATAL', 'INFO', 'WARN'],\n",
    "        default='INFO')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Set python level verbosity\n",
    "    tf.logging.set_verbosity(args.verbosity)\n",
    "    # Set C++ Graph Execution level verbosity\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = str(\n",
    "        tf.logging.__dict__[args.verbosity] / 10)\n",
    "\n",
    "    # Run the training job\n",
    "    hparams = hparam.HParams(**args.__dict__)\n",
    "    train_and_evaluate(hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Cloud MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Single Instance training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "DATE=`date '+%Y%m%d_%H%M%S'`\n",
    "export JOB_NAME=iris_$DATE\n",
    "export GCS_JOB_DIR=gs://iris-dataset/jobs/$JOB_NAME\n",
    "export TRAIN_FILE=gs://iris-dataset/train_data.csv\n",
    "export EVAL_FILE=gs://iris-dataset/test_data.csv\n",
    "\n",
    "echo $GCS_JOB_DIR\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "                                    --stream-logs \\\n",
    "                                    --runtime-version 1.8 \\\n",
    "                                    --job-dir $GCS_JOB_DIR \\\n",
    "                                    --module-name trainer.task \\\n",
    "                                    --package-path trainer/ \\\n",
    "                                    --region us-central1 \\\n",
    "                                    -- \\\n",
    "                                    --train-files $TRAIN_FILE \\\n",
    "                                    --eval-files $EVAL_FILE \\\n",
    "                                    --train-steps 5000 \\\n",
    "                                    --eval-steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ./scripts/single-instance-training.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Distributed training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "export SCALE_TIER=STANDARD_1 # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1 | BASIC_TPU \n",
    "DATE=`date '+%Y%m%d_%H%M%S'`\n",
    "export JOB_NAME=iris_$DATE\n",
    "export GCS_JOB_DIR=gs://iris-dataset/jobs/$JOB_NAME\n",
    "export TRAIN_FILE=gs://iris-dataset/train_data.csv\n",
    "export EVAL_FILE=gs://iris-dataset/test_data.csv\n",
    "\n",
    "echo $GCS_JOB_DIR\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "                                    --stream-logs \\\n",
    "                                    --scale-tier $SCALE_TIER \\\n",
    "                                    --runtime-version 1.8 \\\n",
    "                                    --job-dir $GCS_JOB_DIR \\\n",
    "                                    --module-name trainer.task \\\n",
    "                                    --package-path trainer/ \\\n",
    "                                    --region us-central1 \\\n",
    "                                    -- \\\n",
    "                                    --train-files $TRAIN_FILE \\\n",
    "                                    --eval-files $EVAL_FILE \\\n",
    "                                    --train-steps 5000 \\\n",
    "                                    --eval-steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ./scripts/distributed-training.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running a Distributed training job with Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hptuning\\_config.yaml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%writefile hptuning_config.yaml\n",
    "trainingInput:\n",
    "  hyperparameters:\n",
    "    goal: MAXIMIZE\n",
    "    hyperparameterMetricTag: accuracy\n",
    "    maxTrials: 4\n",
    "    maxParallelTrials: 2\n",
    "    params:\n",
    "      - parameterName: learning-rate\n",
    "        type: DOUBLE\n",
    "        minValue: 0.00001\n",
    "        maxValue: 0.005\n",
    "        scaleType: UNIT_LOG_SCALE\n",
    "      - parameterName: first-layer-size\n",
    "        type: INTEGER\n",
    "        minValue: 50\n",
    "        maxValue: 500\n",
    "        scaleType: UNIT_LINEAR_SCALE\n",
    "      - parameterName: num-layers\n",
    "        type: INTEGER\n",
    "        minValue: 1\n",
    "        maxValue: 15\n",
    "        scaleType: UNIT_LINEAR_SCALE\n",
    "      - parameterName: scale-factor\n",
    "        type: DOUBLE\n",
    "        minValue: 0.1\n",
    "        maxValue: 1.0\n",
    "        scaleType: UNIT_REVERSE_LOG_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run distributed training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "export SCALE_TIER=STANDARD_1 # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1 | BASIC_TPU \n",
    "DATE=`date '+%Y%m%d_%H%M%S'`\n",
    "export JOB_NAME=iris_$DATE\n",
    "export HPTUNING_CONFIG=hptuning_config.yaml\n",
    "export GCS_JOB_DIR=gs://iris-dataset/jobs/$JOB_NAME\n",
    "export TRAIN_FILE=gs://iris-dataset/train_data.csv\n",
    "export EVAL_FILE=gs://iris-dataset/test_data.csv\n",
    "\n",
    "echo $GCS_JOB_DIR\n",
    "\n",
    "gcloud ml-engine jobs submit training $JOB_NAME \\\n",
    "                                    --stream-logs \\\n",
    "                                    --scale-tier $SCALE_TIER \\\n",
    "                                    --runtime-version 1.8 \\\n",
    "                                    --config $HPTUNING_CONFIG \\\n",
    "                                    --job-dir $GCS_JOB_DIR \\\n",
    "                                    --module-name trainer.task \\\n",
    "                                    --package-path trainer/ \\\n",
    "                                    --region us-central1 \\\n",
    "                                    -- \\\n",
    "                                    --train-files $TRAIN_FILE \\\n",
    "                                    --eval-files $EVAL_FILE \\\n",
    "                                    --train-steps 5000 \\\n",
    "                                    --eval-steps 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source ./scripts/hyper-tune.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under `Training output`, the first `trialID` contains the hyper-parameter set that minimizes the cost function and performs best on the evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the Model for making Predictions on Cloud MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "export MODEL_VERSION=v1\n",
    "export MODEL_NAME=iris\n",
    "export MODEL_BINARIES=$GCS_JOB_DIR/3/export/iris/1542241126\n",
    "\n",
    "# Create a Cloud ML Engine model\n",
    "gcloud ml-engine models create $MODEL_NAME\n",
    "\n",
    "# Create a model version\n",
    "gcloud ml-engine versions create $MODEL_VERSION \\\n",
    "    --model $MODEL_NAME \\\n",
    "    --origin $MODEL_BINARIES \\\n",
    "    --runtime-version 1.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "export JOB_NAME=iris_prediction\n",
    "export MODEL_NAME=iris\n",
    "export MODEL_VERSION=v1\n",
    "export TEST_FILE=gs://iris-dataset/hold_out_test.csv\n",
    "\n",
    "# submit a batched job\n",
    "gcloud ml-engine jobs submit prediction $JOB_NAME \\\n",
    "        --model $MODEL_NAME \\\n",
    "        --version $MODEL_VERSION \\\n",
    "        --data-format TEXT \\\n",
    "        --region $REGION \\\n",
    "        --input-paths $TEST_FILE \\\n",
    "        --output-path $GCS_JOB_DIR/predictions\n",
    "\n",
    "# stream job logs\n",
    "echo \"Job logs...\"\n",
    "gcloud ml-engine jobs stream-logs $JOB_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://superconductor/jobs/superconductor_prediction/predictions/ {ENTER_JOB_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%bash\n",
    "# read output summary\n",
    "echo \"Job output summary:\"\n",
    "gsutil cat 'gs://superconductor/jobs/superconductor_prediction/predictions/prediction.results-00000-of-00002'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(’gs://superconductor/jobs/superconductor_181222_040429’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

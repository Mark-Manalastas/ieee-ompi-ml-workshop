{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Dataflow\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "- [Beam Programming](#beam-programming)\n",
    "- [Building a Simple Data Transformation Pipeline](#building-a-simple-data-transformation-pipeline)\n",
    "\n",
    "\n",
    "Google Cloud Dataflow provides a serverless, parallel and distributed infrastructure for running jobs for batch and stream data processing.\n",
    "One of the core strengths of Dataflow is its ability to almost seamlessly handle the switch from processing of batch historical data to streaming datasets while elegantly taking into consideration the perks of streaming processing such as windowing.\n",
    "Dataflow is a major component for building an end-to-end ML production pipeline on GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"beam-programming\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam Programming\n",
    "Apache Beam provides a set of broad concepts to simplify the process of building a transformation pipeline for distributed batch and stream jobs.\n",
    "\n",
    "- **A Pipeline:** A Pipeline object wraps the entire operation and prescribes the transformation process by defining the input data source to the pipeline, how that data will be transformed and where the data will be written.\n",
    "- **A PCollection:** A PCollection is used to define a data source. The data source can either be bounded or unbounded. A bounded data source referes to batch or historical data, whereas an unbounded data source refers to streaming data.\n",
    "- **A PTransform:** PTransforms refers to a particular transformation task carried out on one or more PCollections in the pipeline. A number of core Beam transforms include:\n",
    "  - ParDo: for parallel processing.\n",
    "  - GroupByKey: for processing collections of key/value pairs.\n",
    "  - CoGroupByKey: for a relational join of two or more key/value PCollections with the same key type.\n",
    "  - Combine: for combining collections of elements or values in your data.\n",
    "  - Flatten: for merging multiple PCollection objects.\n",
    "  - Partition: splits a single PCollection into smaller collections. \n",
    "- **I/O Transforms:** These are PTransforms that read or write data to different external storage systems.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/dataflow-sequential-transform.png\" style=\"float:left;\" alt=\"A Simple Linear Pipeline with Sequential Transforms.\" height=90% width=90% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"enable-dataflow-api\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Dataflow API\n",
    "1. Go to API & Services Dashboard\n",
    "2. Click `Enable API & services`\n",
    "3. Search for `Dataflow API`\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/search-dataflow-api.png\" style=\"float:left;\" alt=\"Search for Dataflow API.\" height=90% width=90% />\n",
    "</div>\n",
    "\n",
    "4. Enable `Dataflow API`\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/enable-dataflow-api.png\" style=\"float:left;\" alt=\"Enable Dataflow API.\" height=90% width=90% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"building-a-simple-data-transformation-pipeline\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Simple Data Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating gs://ieee-ompi-datasets/...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# create bucket\n",
    "gsutil mb gs://ieee-ompi-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Copying from <STDIN>...\n",
      "/ [0 files][    0.0 B/    0.0 B]                                                \r",
      "\r",
      "  0 47.0M    0 81054    0     0  50829      0  0:16:09  0:00:01  0:16:08 50817\r",
      "100 47.0M  100 47.0M    0     0  25.6M      0  0:00:01  0:00:01 --:--:-- 25.6M\n",
      "/ [0 files][ 47.0 MiB/    0.0 B]                                                \r",
      "/ [1 files][    0.0 B/    0.0 B]                                                \r\n",
      "Operation completed over 1 objects.                                              \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# transfer data from Github to the bucket.\n",
    "curl https://raw.githubusercontent.com/dvdbisong/IEEE-Carleton-and-OMPI-Machine-Learning-Workshop/master/data/crypto-markets/crypto-markets.csv | gsutil cp - gs://ieee-ompi-datasets/crypto-markets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the apache beam library and other important setup packages.\n",
    "# restart the session after installing apache beam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source activate py2env\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "conda install -y pytz==2018.4\n",
    "pip install apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import apache_beam as beam\n",
    "from apache_beam.io import ReadFromText\n",
    "from apache_beam.io import WriteToText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation code\n",
    "def run(project, source_bucket, target_bucket):\n",
    "    import csv\n",
    "\n",
    "    options = {\n",
    "        'staging_location': 'gs://ieee-ompi-datasets/staging',\n",
    "        'temp_location': 'gs://ieee-ompi-datasets/temp',\n",
    "        'job_name': 'dataflow-crypto',\n",
    "        'project': project,\n",
    "        'max_num_workers': 24,\n",
    "        'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "        'no_save_main_session': True,\n",
    "        'runner': 'DataflowRunner'\n",
    "      }\n",
    "    options = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    \n",
    "    crypto_dataset = 'gs://{}/crypto-markets.csv'.format(source_bucket)\n",
    "    processed_ds = 'gs://{}/transformed-crypto-bitcoin'.format(target_bucket)\n",
    "\n",
    "    pipeline = beam.Pipeline(options=options)\n",
    "\n",
    "    # 0:slug, 3:date, 5:open, 6:high, 7:low, 8:close\n",
    "    rows = (\n",
    "        pipeline |\n",
    "            'Read from bucket' >> ReadFromText(crypto_dataset) |\n",
    "            'Tokenize as csv columns' >> beam.Map(lambda line: next(csv.reader([line]))) |\n",
    "            'Select columns' >> beam.Map(lambda fields: (fields[0], fields[3], fields[5], fields[6], fields[7], fields[8])) |\n",
    "            'Filter bitcoin rows' >> beam.Filter(lambda row: row[0] == 'bitcoin')\n",
    "        )\n",
    "        \n",
    "    combined = (\n",
    "        rows |\n",
    "            'Write to bucket' >> beam.Map(lambda (slug, date, open, high, low, close): '{},{},{},{},{},{}'.format(\n",
    "                slug, date, open, high, low, close)) |\n",
    "            WriteToText(\n",
    "                file_path_prefix=processed_ds,\n",
    "                file_name_suffix=\".csv\", num_shards=2,\n",
    "                shard_name_template=\"-SS-of-NN\",\n",
    "                header='slug, date, open, high, low, close')\n",
    "        )\n",
    "\n",
    "    pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run pipeline on the cloud\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/oauth2client/contrib/gce.py:99: UserWarning: You have requested explicit scopes to be used with a GCE service account.\n",
      "Using this argument will have no effect on the actual scopes for tokens\n",
      "requested. These scopes are set at VM instance creation time and\n",
      "can't be overridden in the request.\n",
      "\n",
      "  warnings.warn(_SCOPES_WARNING)\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.py:800: BeamDeprecationWarning: options is deprecated since First stable release. References to <pipeline>.options will not be supported\n",
      "  options = pbegin.pipeline.options.view_as(DebugOptions)\n"
     ]
    },
    {
     "ename": "HttpForbiddenError",
     "evalue": "HttpError accessing <https://dataflow.googleapis.com/v1b3/projects/oceanic-sky-230504/locations/us-central1/jobs?alt=json>: response: <{'status': '403', 'content-length': '755', 'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff', 'transfer-encoding': 'chunked', 'vary': 'Origin, X-Origin, Referer', 'server': 'ESF', '-content-encoding': 'gzip', 'cache-control': 'private', 'date': 'Sun, 03 Feb 2019 06:54:38 GMT', 'x-frame-options': 'SAMEORIGIN', 'content-type': 'application/json; charset=UTF-8'}>, content <{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"Dataflow API has not been used in project 506126439013 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataflow.googleapis.com/overview?project=506126439013 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\",\n    \"status\": \"PERMISSION_DENIED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Google developers console API activation\",\n            \"url\": \"https://console.developers.google.com/apis/api/dataflow.googleapis.com/overview?project=506126439013\"\n          }\n        ]\n      }\n    ]\n  }\n}\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mHttpForbiddenError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-250d05346436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mprint\u001b[0m \u001b[0;34m'Run pipeline on the cloud'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'oceanic-sky-230504'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_bucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ieee-ompi-datasets'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_bucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ieee-ompi-datasets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-5c5e162c3b1c>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(project, source_bucket, target_bucket)\u001b[0m\n\u001b[1;32m     40\u001b[0m         )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    403\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_runner_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_fake_coders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m           self._options).run(False)\n\u001b[0m\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mruntime_type_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/pipeline.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    416\u001b[0m       \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/dataflow_runner.pyc\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# raise an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     result = DataflowPipelineResult(\n\u001b[0;32m--> 394\u001b[0;31m         self.dataflow_client.create_job(self.job), self)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;31m# TODO(BEAM-4274): Circular import runners-metrics. Requires refactoring.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/utils/retry.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36mcreate_job\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtemplate_location\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 514\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit_job_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m     logging.info('A template was just created at location %s',\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/utils/retry.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexn\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mretry_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/apiclient.pyc\u001b[0m in \u001b[0;36msubmit_job_description\u001b[0;34m(self, job)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojects_locations_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadStatusCodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       logging.error('HTTP status %d trying to create job'\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apache_beam/runners/dataflow/internal/clients/dataflow/dataflow_v1b3_client.pyc\u001b[0m in \u001b[0;36mCreate\u001b[0;34m(self, request, global_params)\u001b[0m\n\u001b[1;32m    577\u001b[0m       \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetMethodConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Create'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m       return self._RunMethod(\n\u001b[0;32m--> 579\u001b[0;31m           config, request, global_params=global_params)\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     Create.method_config = lambda: base_api.ApiMethodInfo(\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apitools/base/py/base_api.pyc\u001b[0m in \u001b[0;36m_RunMethod\u001b[0;34m(self, method_config, request, global_params, upload, upload_config, download)\u001b[0m\n\u001b[1;32m    729\u001b[0m                 http, http_request, **opts)\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcessHttpResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mProcessHttpResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apitools/base/py/base_api.pyc\u001b[0m in \u001b[0;36mProcessHttpResponse\u001b[0;34m(self, method_config, http_response, request)\u001b[0m\n\u001b[1;32m    735\u001b[0m         return self.__client.ProcessResponse(\n\u001b[1;32m    736\u001b[0m             \u001b[0mmethod_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             self.__ProcessHttpResponse(method_config, http_response, request))\n\u001b[0m",
      "\u001b[0;32m/usr/local/envs/py2env/lib/python2.7/site-packages/apitools/base/py/base_api.pyc\u001b[0m in \u001b[0;36m__ProcessHttpResponse\u001b[0;34m(self, method_config, http_response, request)\u001b[0m\n\u001b[1;32m    602\u001b[0m                                              http_client.NO_CONTENT):\n\u001b[1;32m    603\u001b[0m             raise exceptions.HttpError.FromResponse(\n\u001b[0;32m--> 604\u001b[0;31m                 http_response, method_config=method_config, request=request)\n\u001b[0m\u001b[1;32m    605\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNO_CONTENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;31m# TODO(craigcitro): Find out why _replace doesn't seem to work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpForbiddenError\u001b[0m: HttpError accessing <https://dataflow.googleapis.com/v1b3/projects/oceanic-sky-230504/locations/us-central1/jobs?alt=json>: response: <{'status': '403', 'content-length': '755', 'x-xss-protection': '1; mode=block', 'x-content-type-options': 'nosniff', 'transfer-encoding': 'chunked', 'vary': 'Origin, X-Origin, Referer', 'server': 'ESF', '-content-encoding': 'gzip', 'cache-control': 'private', 'date': 'Sun, 03 Feb 2019 06:54:38 GMT', 'x-frame-options': 'SAMEORIGIN', 'content-type': 'application/json; charset=UTF-8'}>, content <{\n  \"error\": {\n    \"code\": 403,\n    \"message\": \"Dataflow API has not been used in project 506126439013 before or it is disabled. Enable it by visiting https://console.developers.google.com/apis/api/dataflow.googleapis.com/overview?project=506126439013 then retry. If you enabled this API recently, wait a few minutes for the action to propagate to our systems and retry.\",\n    \"status\": \"PERMISSION_DENIED\",\n    \"details\": [\n      {\n        \"@type\": \"type.googleapis.com/google.rpc.Help\",\n        \"links\": [\n          {\n            \"description\": \"Google developers console API activation\",\n            \"url\": \"https://console.developers.google.com/apis/api/dataflow.googleapis.com/overview?project=506126439013\"\n          }\n        ]\n      }\n    ]\n  }\n}\n>"
     ]
    }
   ],
   "source": [
    "# execute transfomation\n",
    "if __name__ == '__main__':\n",
    "  print 'Run pipeline on the cloud'\n",
    "  run(project='oceanic-sky-230504', source_bucket='ieee-ompi-datasets', target_bucket='ieee-ompi-datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2dl]",
   "language": "python",
   "name": "conda-env-py2dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

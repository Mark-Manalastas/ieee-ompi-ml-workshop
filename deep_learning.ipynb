{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "- [The representation challenge](#the-representation-challenge)\n",
    "- [An Inspiration from the Brain](#an-inspiration-from-the-brain)\n",
    "- [The Neural Network Architecture](#the-neural-network-architecture)\n",
    "- [Training the Network](#training-the-network)\n",
    "- [Cost Function or Loss Function](#cost-function-or-loss-function)\n",
    "- [The Backpropagation Algorithm](#the-backpropagation-algorithm)\n",
    "- [Activation Functions](#activation-functions)\n",
    "- [MultiLayer Perceptron (MLP) with Tensorflow Estimator API](#mlp-with-tensorflow-estimator-api)\n",
    "  - [Importing the dataset](#importing-the-dataset)\n",
    "  - [Prepare the dataset for modeling](#prepare-the-dataset-for-modeling)\n",
    "  - [The MultiLayer Perceptron Model](#the-mlp-model)\n",
    "\n",
    "Deep learning is a branch of learning models that extend the Neural Network algorithm. It enables the network algorithm to learn classifiers for complex problems such as computer vision and language modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"the-representation-challenge\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The representation challenge\n",
    "Learning is a non-trivial task. How we learn deep representations as humans are high up there as one of the great enigmas of the world. What we consider trivial and to some others natural is a complex web of fine-grained and intricate processes that indeed have set us apart as unique creations in the universe both seen and unseen.\n",
    "\n",
    "One of the greatest challenges of AI research is to get the computer to understand or to innately decompose structural representations of problems just like a human being would. Deep learning approaches this conudrum by learning the underlying representations, also called the deep representations or the hierarchical representations of the dataset based. That is why deep learning is also called representation learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"an-inspiration-from-the-brain\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Inspiration from the Brain\n",
    "A neuron is an autonomous agent in the brain and is a central part of the nervous system. Neurons are responsible for receiving and transmitting information to other cells within the body based on external or internal stimuli. Neurons react by firing electrical impulses generated at the stimuli source to the brain and other cells for the appropriate response. The intricate and coordinated workings of neurons are central to human intelligence.\n",
    "\n",
    "The following are the three most important components of neurons that are of primary interest to us:\n",
    "- The Axon,\n",
    "- The Dendrite, and\n",
    "- The Synapse.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/brain.png\" style=\"float:left;\" alt=\"A Neuron.\" height=90% width=90% />\n",
    "</div>\n",
    "\n",
    "Building on the inspiration of the biological neuron, the artificial neural network (ANN) is a society of connectionist agents that learn and transfer information from one artificial neuron to the other. As data transfers between neurons, a hierarchy of representations or a hierarchy of features is learned. Hence the name deep representation learning or deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"neural-network-architecture\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network Architecture\n",
    "An artificial neural network is composed of:\n",
    "- An input layer,\n",
    "- Hidden layer(s), and\n",
    "- An output layer.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/basic-NN.png\" style=\"float:left;\" alt=\"Neural Network Architecture.\" height=50% width=50% />\n",
    "</div>\n",
    "\n",
    "The input layer receives information from the features of the dataset, some computation takes place, and data propagates to the hidden layer(s).\n",
    "The hidden layer(s) are the workhorse of deep neural networks. They consist of multiple neuron modules where each hidden network layer learns a more sophisticated set of feature representations. The decision on the number of neurons in a layer (network width) and the number of hidden layers (network depth) which forms the network topology are all design choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"training-the-network\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network\n",
    "A weight is assigned to every neuron in the network. They control the activations as information moves from one neural layer to the next. The weights (also called parameters) are initially initialized as a random value but are later adjusted as the network begins to learn via the backpropagation algorithm.\n",
    "\n",
    "Hence, the activations of the neurons in the next layer are determined by the sum of the neuronâ€™s weight times the activations in the previous layer acted upon by a non-linear activation function. Every neuron layer also has a bias neuron that controls the weighted sum. This is similar to the bias term in the logistic regression model.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/information-flow.png\" style=\"float:left;\" alt=\"Information flowing from a previous neural layer to a neuron in the next layer.\" height=80% width=80% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cost-function-or-loss-function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function or Loss Function\n",
    "The quadratic cost which is also known as the mean squared error or the maximum likelihood estimate finds the sum of the difference between the estimated probability and the actual class label - used for regression problems. The cross-entropy cost function, also called the negative log-likelihood or binary cross-entropy, increases as the predicted probability estimates differ from the actual class label in a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"the-backpropagation-algorithm\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Backpropagation Algorithm\n",
    "Backpropagation is an algorithm for training the neural network to get better at improving its predicted outcomes by adjusting the weights of the network. The first time we run the feedforward algorithm, the activations at the output layer are most likely incorrect with a high error estimate or cost function. The goal of backpropagation is to repeatedly go back and adjust the weights of each preceding neural layer and perform the feedforward algorithm again until we minimize the error made by the network at the output layer.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/backpropagation.png\" style=\"float:left;\" alt=\"Backpropagation.\" height=80% width=80% />\n",
    "</div>\n",
    "\n",
    "The cost function at the output layer is obtained by comparing the predicted output of the neural network with the actual outputs from the dataset. Gradient descent (earlier discussed) then computes the gradient of the costs using the weights of the neurons at each successive layer and updating the weights as it propagates back through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"activation-functions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "Activation functions operate on the neurons affine transformations (which is nothing more than the sum of weights and their added bias) by passing it through a non-linear function to decide if that neuron should fire or propagate its information to the succeeding neural layers.\n",
    "\n",
    "In other words, an activation function determines if a particular neuron has the information to result in a correct prediction at the output layer. Activation functions are analogous to how neurons communicate and transfer information in the brain, by firing when the activation goes beyond a particular threshold.\n",
    "\n",
    "These activation functions are also called non-linearities because they inject non-linear capabilities to our network and can learn a mapping from inputs to output for a dataset whose fundamental structure is non-linear.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/activation-function.png\" style=\"float:left;\" alt=\"Activation Function.\" height=80% width=80% />\n",
    "</div>\n",
    "\n",
    "They are various activations function we can use in neural networks, popular options include:\n",
    "- Sigmoid\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/sigmoid.png\" style=\"float:left;\" alt=\"Sigmoid Function.\" height=50% width=50% />\n",
    "</div>\n",
    "- Hyperbolic Tangent (tanh)\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/tanh.png\" style=\"float:left;\" alt=\"Tanh Function.\" height=50% width=50% />\n",
    "</div>\n",
    "- Rectified Linear Unit (ReLU)\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/ReLU.png\" style=\"float:left;\" alt=\"ReLU Function.\" height=50% width=50% />\n",
    "</div>\n",
    "- Leaky ReLU\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/Leaky-ReLU.png\" style=\"float:left;\" alt=\"Lealy ReLU Function.\" height=50% width=50% />\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mlp-with-tensorflow-estimator-api\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiLayer Perceptron (MLP) with Tensorflow Estimator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"importing-the-dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset\n",
    "The dataset used in this example is from the [Climate Model Simulation Crashes Data Set ](https://archive.ics.uci.edu/ml/datasets/climate+model+simulation+crashes). The dataset contains 540 observations and 21 variables ordered as follows:\n",
    "- Column 1: Latin hypercube study ID (study 1 to study 3)  \n",
    "- Column 2: simulation ID (run 1 to run 180)\n",
    "- Columns 3-20: values of 18 climate model parameters scaled in the interval [0, 1]\n",
    "- Column 21: simulation outcome (0 = failure, 1 = success)\n",
    "\n",
    "The goal is to predict climate model simulation outcomes (column 21, fail or succeed) given scaled values of climate model input parameters (columns 3-20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = np.loadtxt(\"data/climate-simulation/pop_failures.dat\", skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.85903621, 0.92782454, 0.25286562,\n",
       "        0.29883831, 0.1705213 , 0.73593604, 0.42832543, 0.56794694,\n",
       "        0.4743696 , 0.24567485, 0.10422587, 0.8690907 , 0.9975185 ,\n",
       "        0.44862008, 0.30752179, 0.85831037, 0.79699724, 0.86989304,\n",
       "        0.        ],\n",
       "       [1.        , 2.        , 0.60604103, 0.45772836, 0.35944842,\n",
       "        0.30695738, 0.84333077, 0.93485066, 0.44457249, 0.82801493,\n",
       "        0.29661775, 0.6168699 , 0.97578558, 0.91434367, 0.84524714,\n",
       "        0.86415187, 0.34671269, 0.35657342, 0.43844719, 0.51225614,\n",
       "        1.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview data\n",
    "data[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540, 21)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of rows and columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare-the-dataset-for-modeling\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target\n",
    "X = data[:,:-1]\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 1.        , 0.85903621, 0.92782454, 0.25286562,\n",
       "        0.29883831, 0.1705213 , 0.73593604, 0.42832543, 0.56794694,\n",
       "        0.4743696 , 0.24567485, 0.10422587, 0.8690907 , 0.9975185 ,\n",
       "        0.44862008, 0.30752179, 0.85831037, 0.79699724, 0.86989304],\n",
       "       [1.        , 2.        , 0.60604103, 0.45772836, 0.35944842,\n",
       "        0.30695738, 0.84333077, 0.93485066, 0.44457249, 0.82801493,\n",
       "        0.29661775, 0.6168699 , 0.97578558, 0.91434367, 0.84524714,\n",
       "        0.86415187, 0.34671269, 0.35657342, 0.43844719, 0.51225614],\n",
       "       [1.        , 3.        , 0.99759978, 0.37323849, 0.51739936,\n",
       "        0.50499255, 0.61890334, 0.60557082, 0.74622533, 0.19592829,\n",
       "        0.81566694, 0.67935503, 0.80341308, 0.64399516, 0.71844113,\n",
       "        0.92477507, 0.31537141, 0.25064237, 0.28563553, 0.36585796]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample of features\n",
    "X[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# targets\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-mlp-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MultiLayer Perceptron Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get column names\n",
    "col_names = pd.read_csv(\"data/climate-simulation/pop_failures.dat\", nrows=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_lst=list(col_names.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_str=re.sub(\"\\s+\", \",\", names_lst[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[x.strip() for x in names_str.split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Study', 'Run', 'vconst_corr', 'vconst_2', 'vconst_3', 'vconst_4', 'vconst_5', 'vconst_7', 'ah_corr', 'ah_bolus', 'slm_corr', 'efficiency_factor', 'tidal_mix_max', 'vertical_decay_scale', 'convect_corr', 'bckgrnd_vdc1', 'bckgrnd_vdc_ban', 'bckgrnd_vdc_eq', 'bckgrnd_vdc_psim', 'Prandtl', 'outcome']\n",
      "\n",
      "length: 21\n"
     ]
    }
   ],
   "source": [
    "# column names\n",
    "print(names)\n",
    "print(\"\\nlength:\", len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input_fn\n",
    "def input_fn(X, y, batch_size=30, training=True):\n",
    "    # convert to dictionary\n",
    "    X_dict={}\n",
    "    for ind, name in enumerate(names[:-1]):\n",
    "        X_dict[name]=X[:,ind]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X_dict, y))\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use feature columns to define the attributes to the model\n",
    "Study = tf.feature_column.numeric_column('Study')\n",
    "Run = tf.feature_column.numeric_column('Run')\n",
    "vconst_corr = tf.feature_column.numeric_column('vconst_corr')\n",
    "vconst_2 = tf.feature_column.numeric_column('vconst_2')\n",
    "vconst_3 = tf.feature_column.numeric_column('vconst_3')\n",
    "vconst_4 = tf.feature_column.numeric_column('vconst_4')\n",
    "vconst_5 = tf.feature_column.numeric_column('vconst_5')\n",
    "vconst_7 = tf.feature_column.numeric_column('vconst_7')\n",
    "ah_corr = tf.feature_column.numeric_column('ah_corr')\n",
    "ah_bolus = tf.feature_column.numeric_column('ah_bolus')\n",
    "slm_corr = tf.feature_column.numeric_column('slm_corr')\n",
    "efficiency_factor = tf.feature_column.numeric_column('efficiency_factor')\n",
    "tidal_mix_max = tf.feature_column.numeric_column('tidal_mix_max')\n",
    "vertical_decay_scale = tf.feature_column.numeric_column('vertical_decay_scale')\n",
    "convect_corr = tf.feature_column.numeric_column('convect_corr')\n",
    "bckgrnd_vdc1 = tf.feature_column.numeric_column('bckgrnd_vdc1')\n",
    "bckgrnd_vdc_ban = tf.feature_column.numeric_column('bckgrnd_vdc_ban')\n",
    "bckgrnd_vdc_eq = tf.feature_column.numeric_column('bckgrnd_vdc_eq')\n",
    "bckgrnd_vdc_psim = tf.feature_column.numeric_column('bckgrnd_vdc_psim')\n",
    "Prandtl = tf.feature_column.numeric_column('Prandtl')\n",
    "feature_columns = [Study, Run, vconst_corr, vconst_2, vconst_3, vconst_4, vconst_5, vconst_7, convect_corr,\n",
    "                  ah_corr, ah_bolus, slm_corr, efficiency_factor, tidal_mix_max, vertical_decay_scale,\n",
    "                  bckgrnd_vdc1, bckgrnd_vdc_ban, bckgrnd_vdc_eq, bckgrnd_vdc_psim, Prandtl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmp4mah8k5o\n",
      "INFO:tensorflow:Using config: {'_num_worker_replicas': 1, '_num_ps_replicas': 0, '_eval_distribute': None, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1331449b0>, '_evaluation_master': '', '_global_id_in_cluster': 0, '_experimental_distribute': None, '_log_step_count_steps': 100, '_save_summary_steps': 100, '_service': None, '_train_distribute': None, '_is_chief': True, '_task_type': 'worker', '_keep_checkpoint_max': 5, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_protocol': None, '_task_id': 0, '_save_checkpoints_steps': None, '_device_fn': None, '_model_dir': '/var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmp4mah8k5o', '_master': '', '_save_checkpoints_secs': 600, '_keep_checkpoint_every_n_hours': 10000}\n"
     ]
    }
   ],
   "source": [
    "# instantiate a DNNClassifier Estimator\n",
    "estimator = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,\n",
    "    optimizer='Adam',\n",
    "    hidden_units=[1024, 512, 256],\n",
    "    activation_fn=tf.nn.relu,\n",
    "    n_classes=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmp4mah8k5o/model.ckpt.\n",
      "INFO:tensorflow:loss = 12.81837, step = 1\n",
      "INFO:tensorflow:global_step/sec: 153.705\n",
      "INFO:tensorflow:loss = 7.897155, step = 101 (0.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 222.691\n",
      "INFO:tensorflow:loss = 7.369482, step = 201 (0.449 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.702\n",
      "INFO:tensorflow:loss = 7.373659, step = 301 (0.422 sec)\n",
      "INFO:tensorflow:global_step/sec: 226.485\n",
      "INFO:tensorflow:loss = 12.280007, step = 401 (0.442 sec)\n",
      "INFO:tensorflow:global_step/sec: 200.164\n",
      "INFO:tensorflow:loss = 9.850716, step = 501 (0.499 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.473\n",
      "INFO:tensorflow:loss = 7.3498316, step = 601 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 227.987\n",
      "INFO:tensorflow:loss = 2.8503788, step = 701 (0.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 213.14\n",
      "INFO:tensorflow:loss = 16.974874, step = 801 (0.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.459\n",
      "INFO:tensorflow:loss = 7.389231, step = 901 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 210.224\n",
      "INFO:tensorflow:loss = 16.682652, step = 1001 (0.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 202.052\n",
      "INFO:tensorflow:loss = 5.142111, step = 1101 (0.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 233.348\n",
      "INFO:tensorflow:loss = 7.4850435, step = 1201 (0.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 232.342\n",
      "INFO:tensorflow:loss = 2.9581068, step = 1301 (0.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.923\n",
      "INFO:tensorflow:loss = 7.57767, step = 1401 (0.435 sec)\n",
      "INFO:tensorflow:global_step/sec: 229.363\n",
      "INFO:tensorflow:loss = 5.0159426, step = 1501 (0.436 sec)\n",
      "INFO:tensorflow:global_step/sec: 241.49\n",
      "INFO:tensorflow:loss = 4.823575, step = 1601 (0.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 234.74\n",
      "INFO:tensorflow:loss = 9.993468, step = 1701 (0.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 236.354\n",
      "INFO:tensorflow:loss = 7.405856, step = 1801 (0.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 231.335\n",
      "INFO:tensorflow:loss = 7.3990912, step = 1901 (0.432 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmp4mah8k5o/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 5.0065947.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1331449e8>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "estimator.train(input_fn=lambda:input_fn(X_train, y_train), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to \"careful_interpolation\" instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-02-01-20:28:43\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmp4mah8k5o/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-02-01-20:28:44\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.90123457, accuracy_baseline = 0.90123457, auc = 0.49999994, auc_precision_recall = 0.9506173, average_loss = 0.3239939, global_step = 2000, label/mean = 0.90123457, loss = 8.747835, precision = 0.90123457, prediction/mean = 0.9173998, recall = 1.0\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmp4mah8k5o/model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "metrics = estimator.evaluate(input_fn=lambda:input_fn(X_test, y_test, training=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 0.901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py2dl]",
   "language": "python",
   "name": "conda-env-py2dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

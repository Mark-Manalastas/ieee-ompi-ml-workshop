{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with TensorFlow\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "- [The Logit or Sigmoid Model](#the-logit-or-sigmoid-model)\n",
    "- [Logistic Regression Cost Function](#logistic-regression-cost-function)\n",
    "- [Logistic Regression Model with TensorFlow Canned Estimators](#logistic-regression-model-with-tensorFlow-canned-estimators)\n",
    "  - [Tensorflow Datasets (tf.data)](#tensorflow-datasets)\n",
    "  - [FeatureColumns](#featurecolumns)\n",
    "  - [Estimators](#estimators)\n",
    "\n",
    "Logistic regression is a supervised machine learning algorithm developed for learning classification problems.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/logistic-table.png\" style=\"float:left;\" alt=\"Dataset with categorical outputs.\" height=40% width=40% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"the-logit-or-sigmoid-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Logit or Sigmoid Model\n",
    "The logistic function, also known as logit or sigmoid function constrains the output of the cost function as a probability between 0 and 1. The sigmoid function is formally written as:\n",
    "\n",
    "$$h(t)=\\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "Logistic regression is also parametric as shown below:\n",
    "\n",
    "$$\\hat{y}=\\theta_{0}+\\theta_{1}x_{1}+\\theta_{2}x_{2}+...+\\theta_{n}x_{n}$$\n",
    "\n",
    "where, $$0\\leq h(t)\\leq1.$$ The sigmoid function is illustrated below:\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/sigmoid-function.png\" style=\"float:left;\" alt=\"Logistic function.\" height=75% width=75% />\n",
    "</div>\n",
    "\n",
    "The sigmoid function, resembing an $S$ curve, rises from 0 and plateaus at 1. As $X_1$ increases to infinity the sigmoid output gets closer to 1, and as $X_1$ decreases towards negative infinity, the sigmoid function outputs 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistic-regression-cost-function\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Cost Function\n",
    "The logistic regression cost function is formally written as:\n",
    "\n",
    "$$Cost(h(t),\\;y)=\\begin{cases}\n",
    "-log(h(t)) & \\text{if y=1}\\\\\n",
    "-log(1-h(t)) & \\text{if y=0}\n",
    "\\end{cases}$$\n",
    "\n",
    "The cost function also known as log-loss, is set up in this form to output the penalty made on the algorithm if $h(t)$ predicts one class, and the actual output is another.\n",
    "\n",
    "<div style=\"display: inline-block;width: 100%;\">\n",
    "<img src=\"ieee-ompi/logit-y-1.png\" style=\"float:left;\" alt=\"Logistic function.\" height=35% width=35% />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"logistic-regression-model-with-tensorFlow-canned-estimators\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model with TensorFlow Canned Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tensorflow-datasets\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Datasets (tf.data)\n",
    "The Datasets package (`tf.data`) provides a convenient set of high-level functions for creating complex dataset input pipelines. The goal of the Dataset package is to have a fast, flexible and easy to use interface for fetching data from various data sources, performing data transform operations on them before passing them as inputs to the learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input_fn\n",
    "def input_fn(X, y, batch_size=30, training=True):\n",
    "    # convert to dictionary\n",
    "    X = {'sepal_length': X[:,0],\n",
    "         'sepal_width':  X[:,1],\n",
    "         'petal_length': X[:,2],\n",
    "         'petal_width':  X[:,3]}\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, y))\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"featurecolumns\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureColumns\n",
    "TensorFlow offers a high-level API called FeatureColumns `tf.feature_column` for describing the features of the dataset that will be fed into an Estimator for training and validation. This makes easy the preparation of data for modeling, such as the conversion of categorical features of the dataset into a one-hot encoded vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use feature columns to define the attributes to the model\n",
    "sepal_length = tf.feature_column.numeric_column('sepal_length')\n",
    "sepal_width = tf.feature_column.numeric_column('sepal_width')\n",
    "petal_length = tf.feature_column.numeric_column('petal_length')\n",
    "petal_width = tf.feature_column.numeric_column('petal_width')\n",
    "feature_columns = [sepal_length, sepal_width, petal_length, petal_width]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"estimators\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimators\n",
    "The Estimator API is a high-level TensorFlow functionality that is aimed at reducing the complexity involved in building machine learning models by exposing methods that abstract common models and processes. There are two ways of working with Estimators and they include:\n",
    "- Using the Pre-made Estimators: The pre-made Estimators, are black-box models for building common machine learning and deep learning architectures such as Linear Regression/ Classification, Random Forests Regression/ Classification and Deep Neural Networks for regression and classification.\n",
    "- Creating a Custom Estimator: It is also possible to use the low-level TensorFlow methods to create a custom black-box model for easy reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmpfh2isgdm\n",
      "INFO:tensorflow:Using config: {'_is_chief': True, '_task_id': 0, '_tf_random_seed': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_evaluation_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x12ae30748>, '_keep_checkpoint_every_n_hours': 10000, '_train_distribute': None, '_save_summary_steps': 100, '_device_fn': None, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_global_id_in_cluster': 0, '_log_step_count_steps': 100, '_experimental_distribute': None, '_save_checkpoints_secs': 600, '_service': None, '_model_dir': '/var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmpfh2isgdm', '_eval_distribute': None, '_master': '', '_num_worker_replicas': 1, '_task_type': 'worker', '_protocol': None}\n"
     ]
    }
   ],
   "source": [
    "# instantiate a DNNLinearCombinedClassifier Estimator\n",
    "estimator = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    dnn_feature_columns=feature_columns,\n",
    "    dnn_optimizer='Adam',\n",
    "    dnn_hidden_units=[20],\n",
    "    dnn_activation_fn=tf.nn.relu,\n",
    "    n_classes=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmpfh2isgdm/model.ckpt.\n",
      "INFO:tensorflow:loss = 81.23317, step = 1\n",
      "INFO:tensorflow:global_step/sec: 600.654\n",
      "INFO:tensorflow:loss = 27.36479, step = 101 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 940.718\n",
      "INFO:tensorflow:loss = 18.951374, step = 201 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 950.036\n",
      "INFO:tensorflow:loss = 13.503283, step = 301 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 911.861\n",
      "INFO:tensorflow:loss = 13.111652, step = 401 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 962.557\n",
      "INFO:tensorflow:loss = 11.065907, step = 501 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 959.398\n",
      "INFO:tensorflow:loss = 12.336379, step = 601 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 940.425\n",
      "INFO:tensorflow:loss = 9.40949, step = 701 (0.106 sec)\n",
      "INFO:tensorflow:global_step/sec: 934.064\n",
      "INFO:tensorflow:loss = 9.737815, step = 801 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 990.931\n",
      "INFO:tensorflow:loss = 6.629754, step = 901 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 992.763\n",
      "INFO:tensorflow:loss = 7.085542, step = 1001 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 928.204\n",
      "INFO:tensorflow:loss = 4.734715, step = 1101 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 991.022\n",
      "INFO:tensorflow:loss = 4.697848, step = 1201 (0.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 983.922\n",
      "INFO:tensorflow:loss = 5.4243565, step = 1301 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 966.659\n",
      "INFO:tensorflow:loss = 4.257612, step = 1401 (0.103 sec)\n",
      "INFO:tensorflow:global_step/sec: 928.644\n",
      "INFO:tensorflow:loss = 3.7373846, step = 1501 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 953.125\n",
      "INFO:tensorflow:loss = 3.765056, step = 1601 (0.105 sec)\n",
      "INFO:tensorflow:global_step/sec: 925.574\n",
      "INFO:tensorflow:loss = 2.456142, step = 1701 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 965.073\n",
      "INFO:tensorflow:loss = 2.9977791, step = 1801 (0.104 sec)\n",
      "INFO:tensorflow:global_step/sec: 969.002\n",
      "INFO:tensorflow:loss = 2.297768, step = 1901 (0.103 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmpfh2isgdm/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.8454885.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn_linear_combined.DNNLinearCombinedClassifier at 0x12ae30080>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "estimator.train(input_fn=lambda:input_fn(X_train, y_train), steps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-01-31-17:00:28\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmpfh2isgdm/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-01-31-17:00:28\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.9736842, average_loss = 0.090503275, global_step = 2000, loss = 1.7195622\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2000: /var/folders/gh/mqsbbqy55bb4z763xxgddw780000gn/T/tmpfh2isgdm/model.ckpt-2000\n"
     ]
    }
   ],
   "source": [
    "# evaluate model\n",
    "metrics = estimator.evaluate(input_fn=lambda:input_fn(X_test, y_test, training=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 0.974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**metrics))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pydl]",
   "language": "python",
   "name": "conda-env-pydl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
